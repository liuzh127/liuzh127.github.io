---
layout: post
title: ELK安装
categories: [Linux]
tags: [ELK]
---
#### 1. 环境配置
##### 1.1 创建elk用户并设置elk用户的密码
**elasticsearch默认不能在root用户下运行，所以需新创建一个elk用户**  
`# groupadd elk`   
`# useradd -g elk -m elk`  
`# passwd elk`  
<!-- more -->
##### 1.2 设置Linux环境参数
**修改limits.conf**  
*limits.conf是会话级别，退出会话重新登录需改即可生效。*  
`# vim /etc/security/limits.conf`  
```
* soft nofile 65536  
* hard nofile 131072  
* soft nproc 2048  
* hard nproc 4096  
```
**修改20-nproc.conf**  
*某些版本linux的max user processes配置来源是20-nproc.conf，还需修改20-nproc.conf（centos6是90-nproc.conf）。*  
`# vim /etc/security/limits.d/20-nproc.conf`  
`* soft nproc 4096`  
**修改sysctl.conf**  
`# vim /etc/sysctl.conf`  
`vm.max_map_count=655360`  
`# sysctl -p`  
##### 1.3 Java环境配置 
下载[Java源码](http://jdk.java.net/archive/)并上传到目标机器后执行下面命令：  
`# mkdir /usr/local/java`   
`# cd /usr/local/java`   
`# tar xzf openjdk-12-linux-x64.tar.gz`   
`# alternatives --install /usr/bin/java java /usr/local/jdk-12/bin/java 2`   
`# alternatives --config java`   
```
There are 3 programs which provide 'java'.
Selection Command
-----------------------------------------------
* 1 /opt/jdk1.7.0_71/bin/java
+ 2 /opt/jdk1.8.0_45/bin/java
3 /opt/jdk1.8.0_144/bin/java
4 /usr/local/jdk-12/bin/java
Enter to keep the current selection[+], or type selection number: 4
```
`# vim /etc/profile`  
[在文件最后添加]
```
export JAVA_HOME=/usr/local/jdk-12
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin
export JAVA_HOME CLASSPATH PATH
```
[刷新环境变量]  
`# source /etc/profile`  
#### 2. filebeat安装
下载[filebeat源码](https://www.elastic.co/cn/downloads/past-releases#filebeat)并上传到目标机器后执行下面命令：  
`$ tar -xvzf filebeat-6.7.2-linux-x86_64.tar.gz -C /usr/local/elk`   
`$ cd /usr/local/filebeat-7.4.1-linux-x86_64`  
`$ vim filebeat.yml`  
```
filebeat.prospectors:
[government_debug日志]
- type: log
  enabled: true
  paths:
    - /usr/local/apache-tomcat-8.5.31/logs/debug.log
  fields:
    log_topics: es1_debug

[government_info日志]
- type: log
  enabled: true
  paths:
    - /usr/local/apache-tomcat-8.5.31/logs/info.log
  fields:
    log_topics: es1_info

[government_warn日志]
- type: log
  enabled: true
  paths:
    - /usr/local/apache-tomcat-8.5.31/logs/warn.log
  fields:
    log_topics: es1_warn

[government_error日志]
- type: log
  enabled: true
  paths:
- /usr/local/apache-tomcat-8.5.31/logs/error.log
  fields:
    log_topics: es1_error
[多行日志合并]
  multiline:
       pattern: '^\d{4}-\d{2}-\d{2}\s{1,5}[0-2][0-9]:[0-5][0-9]:[0-5][0-9]'
       negate:  true
       match:   after
       max_lines: 500
       timeout: 10s
  tail_files: true
[日志输出到kafka]
output.kafka:
  enabled: true
  hosts: ["192.168.0.1:9092"]
  topic: "%{[fields][log_topics]}"
```
[启动filebeat]  
`$ cd /usr/local/elk/filebeat-6.7.2-linux-x86_64`  
`$ nohup ./filebeat -e -c filebeat.yml > filebeat.log &`  
[停止filebeat]  
`$ ps -ef|grep filebeat`  
`$ kill -15 pid`  
#### 3. kafka安装
下载[kafka源码](http://kafka.apache.org/downloads)并上传到目标机器后执行下面命令：  
`$ tar -xvzf kafka_2.11-2.0.0.tar.gz -C /usr/local/elk/`  
**zookeeper和kafka的配置保持默认即可**   
[启动zookeeper]  
`$ cd /usr/local/elk/kafka_2.11-2.0.0`  
`$ bin/zookeeper-server-start.sh  config/zookeeper.properties 1>/dev/null 2>&1 &`  
[启动kafka]    
`$ bin/kafka-server-start.sh  config/server.properties 1>/dev/null 2>&1 &`  
[停止zookeeper]  
`$ bin/zookeeper-server-stop.sh  config/zookeeper.properties`  
[停止kafka]  
`$ bin/kafka-server-stop.sh  config/server.properties`  
[查看kafka是否接受到filebeat收集到的日志信息]  
`$ bin/kafka-console-consumer.sh --bootstrap-server 192.168.0.1:9092 --topic test --from-beginning`  
**日志信息会打印到控制台**  
[kafka常用命令]  
创建一个topic：    
`$ cd /usr/local/kafka_2.11-2.0.0`  
`$ bin/kafka-topics.sh --create --zookeeper 192.168.0.1:2181 --replication-factor 1 --partitions 1 --topic test`  
查看所有topic：  
`$ bin/kafka-topics.sh --list --zookeeper 192.168.0.1:2181`  
测试生产消息：  
`$ bin/kafka-console-producer.sh --broker-list 192.168.0.1:9092 --topic test`  
*this is test*  
测试消费消息(另起一个终端执行命令)：  
`$ bin/kafka-console-consumer.sh --bootstrap-server 192.168.0.1:9092 --topic test --from-beginning`  
*this is test*  
#### 4. logstash安装
下载[logstash源码](https://www.elastic.co/cn/downloads/past-releases#logstash)并上传到目标机器后执行下面命令：  
`$ tar -xvzf logstash-6.7.2.tar.gz -C /usr/local/elk`  
`$ cd /usr/local/elk/logstash-6.7.2/config`  
`$ vim logstash.conf`  
```
input {
        kafka {
                bootstrap_servers => "192.168.0.1:9092"
		codec => "json"
		group_id => "es1"
                auto_offset_reset => "earliest"
                consumer_threads => 2
		decorate_events => false
		topics_pattern => "government_.*"
               }
      }
output {
         elasticsearch {
                         hosts => ["192.168.0.1:9200"]
                         index => "logstash"
                       }
       }
```
[修改内存配置]  
`vim config/jvm.options`  
*-Xms256m*  
*-Xmx256m*  
[后台启动logstash]  
`$ nohup bin/logstash -f config/logstash.conf &`  
[停止logstash]  
`$ ps -ef|grep logstash`  
*32026 Logstash*  
`$ kill -15 32026`  
#### 5. elasticsearch安装
下载[elasticsearch源码](https://www.elastic.co/cn/downloads/past-releases#elasticsearch)并上传到目标机器后执行下面命令：  
`$ tar -xzvf elasticsearch-6.7.2.tar.gz`   
`$ cd elasticsearch-6.7.2/config`  
`$ vi elasticsearch.yml`  
```  
cluster.name: log-dig  
node.name: es1  
path.data: /usr/local/elk/elasticsearch-6.7.2/data
path.logs: /usr/local/elasticsearch-6.7.2/logs
[绑定监听IP]
network.host: 192.168.0.1
[设置对外服务的http端口,默认为9200] 
http.port: 9200
```
[启动elasticsearch]    
`$ bin/elasticsearch -d`  
[测试安装是否成功]  
`$ curl 192.168.0.1:9200`  
```
{
  "name" : "es1",
  "cluster_name" : "log-dig",
  "cluster_uuid" : "EFpju3lYSeOL3Vs5bLM81A",
  "version" : {
    "number" : "6.5.3",
    "build_flavor" : "default",
    "build_type" : "tar",
    "build_hash" : "159a78a",
    "build_date" : "2018-12-06T20:11:28.826501Z",
    "build_snapshot" : false,
    "lucene_version" : "7.5.0",
    "minimum_wire_compatibility_version" : "5.6.0",
    "minimum_index_compatibility_version" : "5.0.0"
  },
  "tagline" : "You Know, for Search"
}
```
[查看elasticsearch索引]  
`$ curl 192.168.0.1:9200/_cat/indices?v`  
[停止elasticsearch]  
`$ ps -ef|grep elasticsearch`  
*28410 Elasticsearch*  
`$ kill -15 28410`  
#### 6. kibana安装
下载[kibana源码](https://www.elastic.co/cn/downloads/past-releases#kibana)并上传到目标机器后执行下面命令：  
`$ tar -xzvf kibana-6.7.2-linux-x86_64.tar.gz`  
`$ cd kibana-6.7.2-linux-x86_64/`  
`$ vim config/kibana.yml`   
```
server.name: "elk"
server.host: "192.168.0.1"
server.port: 5601
elasticsearch.url: "http://192.168.0.1:9200"
```
[启动kibana]  
`$ nohup bin/kibana -c config/kibana.yml &`  
[停止kibana]  
`$ ps -ef|grep node`  
*elk   9514   1  1 Dec21 ?   01:30:02 ./../node/bin/node --no-warnings ./../src/cli*  
`# kill -15 9514`  
**验证kibana是否安装成功，访问http://192.168.0.1:5601即可**  
#### 7. 配置示例
[filebeat.yml]  
```
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /home/tomcat/logs/123.log
filebeat.config.modules:
  path: ${path.config}/modules.d/*yml
  reload.enabled: false
setup.template.settings:
  index.number_of_shards: 3
output.kafka:
  enabled: true
  hosts: ["192.168.76.127:9092"]
  topic: "tomcat" 
```
[logstash.conf]  
```
input {
        kafka{
               bootstrap_servers => ["192.168.76.127:9092"]
               group_id =>"baicai"
               auto_offset_reset => "earliest"
               consumer_threads => "5"
               decorate_events => "false"
               topics => ["tomcat"] 
               type => "bbs_logs"
               codec => json
             }
      }
output {    
    elasticsearch { 
        hosts=> "192.168.76.127:9200"
        index=> "elk"
   }
}

```
[elasticsearch.yml]  
```
# ======================== Elasticsearch Configuration =========================
#
# NOTE: Elasticsearch comes with reasonable defaults for most settings.
#       Before you set out to tweak and tune the configuration, make sure you
#       understand what are you trying to accomplish and the consequences.
#
# The primary way of configuring a node is via this file. This template lists
# the most important settings you may want to configure for a production cluster.
#
# Please consult the documentation for further information on configuration options:
# https://www.elastic.co/guide/en/elasticsearch/reference/index.html
#
# ---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster:
#
#cluster.name: my-application
#
# ------------------------------------ Node ------------------------------------
#
# Use a descriptive name for the node:
#
#node.name: node-1
#
# Add custom attributes to the node:
#
#node.attr.rack: r1
#
# ----------------------------------- Paths ------------------------------------
#
# Path to directory where to store the data (separate multiple locations by comma):
#
#path.data: /path/to/data
#
# Path to log files:
#
#path.logs: /path/to/logs
#
# ----------------------------------- Memory -----------------------------------
#
# Lock the memory on startup:
#
#bootstrap.memory_lock: true
#
# Make sure that the heap size is set to about half the memory available
# on the system and that the owner of the process is allowed to use this
# limit.
#
# Elasticsearch performs poorly when the system is swapping the memory.
#
# ---------------------------------- Network -----------------------------------
#
# Set the bind address to a specific IP (IPv4 or IPv6):
#
network.host: 192.168.76.127
#
# Set a custom port for HTTP:
#
http.port: 9200
#
# For more information, consult the network module documentation.
#
# --------------------------------- Discovery ----------------------------------
#
# Pass an initial list of hosts to perform discovery when new node is started:
# The default list of hosts is ["127.0.0.1", "[::1]"]
#
#discovery.zen.ping.unicast.hosts: ["host1", "host2"]
#
# Prevent the "split brain" by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):
#
#discovery.zen.minimum_master_nodes: 
#
# For more information, consult the zen discovery module documentation.
#
# ---------------------------------- Gateway -----------------------------------
#
# Block initial recovery after a full cluster restart until N nodes are started:
#
#gateway.recover_after_nodes: 3
#
# For more information, consult the gateway module documentation.
#
# ---------------------------------- Various -----------------------------------
#
# Require explicit names when deleting indices:
#
#action.destructive_requires_name: true
```
[kibana.yml]  
```
# Kibana is served by a back end server. This setting specifies the port to use.
server.port: 5601

# Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.
# The default is 'localhost', which usually means remote machines will not be able to connect.
# To allow connections from remote users, set this parameter to a non-loopback address.
server.host: "192.168.76.127"

# Enables you to specify a path to mount Kibana at if you are running behind a proxy.
# Use the `server.rewriteBasePath` setting to tell Kibana if it should remove the basePath
# from requests it receives, and to prevent a deprecation warning at startup.
# This setting cannot end in a slash.
#server.basePath: ""

# Specifies whether Kibana should rewrite requests that are prefixed with
# `server.basePath` or require that they are rewritten by your reverse proxy.
# This setting was effectively always `false` before Kibana 6.3 and will
# default to `true` starting in Kibana 7.0.
#server.rewriteBasePath: false

# The maximum payload size in bytes for incoming server requests.
#server.maxPayloadBytes: 1048576

# The Kibana server's name.  This is used for display purposes.
server.name: "es1"

# The URLs of the Elasticsearch instances to use for all your queries.
elasticsearch.hosts: ["http://192.168.76.127:9200"]

# When this setting's value is true Kibana uses the hostname specified in the server.host
# setting. When the value of this setting is false, Kibana uses the hostname of the host
# that connects to this Kibana instance.
#elasticsearch.preserveHost: true

# Kibana uses an index in Elasticsearch to store saved searches, visualizations and
# dashboards. Kibana creates a new index if the index doesn't already exist.
#kibana.index: ".kibana"

# The default application to load.
#kibana.defaultAppId: "home"

# If your Elasticsearch is protected with basic authentication, these settings provide
# the username and password that the Kibana server uses to perform maintenance on the Kibana
# index at startup. Your Kibana users still need to authenticate with Elasticsearch, which
# is proxied through the Kibana server.
#elasticsearch.username: "user"
#elasticsearch.password: "pass"

# Enables SSL and paths to the PEM-format SSL certificate and SSL key files, respectively.
# These settings enable SSL for outgoing requests from the Kibana server to the browser.
#server.ssl.enabled: false
#server.ssl.certificate: /path/to/your/server.crt
#server.ssl.key: /path/to/your/server.key

# Optional settings that provide the paths to the PEM-format SSL certificate and key files.
# These files validate that your Elasticsearch backend uses the same key files.
#elasticsearch.ssl.certificate: /path/to/your/client.crt
#elasticsearch.ssl.key: /path/to/your/client.key

# Optional setting that enables you to specify a path to the PEM file for the certificate
# authority for your Elasticsearch instance.
#elasticsearch.ssl.certificateAuthorities: [ "/path/to/your/CA.pem" ]

# To disregard the validity of SSL certificates, change this setting's value to 'none'.
#elasticsearch.ssl.verificationMode: full

# Time in milliseconds to wait for Elasticsearch to respond to pings. Defaults to the value of
# the elasticsearch.requestTimeout setting.
#elasticsearch.pingTimeout: 1500

# Time in milliseconds to wait for responses from the back end or Elasticsearch. This value
# must be a positive integer.
#elasticsearch.requestTimeout: 30000

# List of Kibana client-side headers to send to Elasticsearch. To send *no* client-side
# headers, set this value to [] (an empty list).
#elasticsearch.requestHeadersWhitelist: [ authorization ]

# Header names and values that are sent to Elasticsearch. Any custom headers cannot be overwritten
# by client-side headers, regardless of the elasticsearch.requestHeadersWhitelist configuration.
#elasticsearch.customHeaders: {}

# Time in milliseconds for Elasticsearch to wait for responses from shards. Set to 0 to disable.
#elasticsearch.shardTimeout: 30000

# Time in milliseconds to wait for Elasticsearch at Kibana startup before retrying.
#elasticsearch.startupTimeout: 5000

# Logs queries sent to Elasticsearch. Requires logging.verbose set to true.
#elasticsearch.logQueries: false

# Specifies the path where Kibana creates the process ID file.
#pid.file: /var/run/kibana.pid

# Enables you specify a file where Kibana stores log output.
#logging.dest: stdout

# Set the value of this setting to true to suppress all logging output.
#logging.silent: false

# Set the value of this setting to true to suppress all logging output other than error messages.
#logging.quiet: false

# Set the value of this setting to true to log all events, including system usage information
# and all requests.
#logging.verbose: false

# Set the interval in milliseconds to sample system and process performance
# metrics. Minimum is 100ms. Defaults to 5000.
#ops.interval: 5000

# Specifies locale to be used for all localizable strings, dates and number formats.
i18n.locale: "zh-CN"
```
